---
title: "hw6"
output: html_document
---



# 2.26
## (a) 

```{r}
X <- c(16, 16, 16, 16, 24, 24, 24, 24, 
       32, 32, 32, 32, 40, 40, 40, 40)
Y <- c(199, 205, 196, 200, 218, 220, 215, 223, 
       237, 234, 235, 230, 250, 248, 253, 246)
n <- 16
b1 <- sum((X-mean(X))*(Y-mean(Y)))/sum((X-mean(X))^2)
b0 <- mean(Y)-mean(X)*b1
Y_hat <- b0+b1*X
sse <- sum((Y-Y_hat)^2)
sse
ssr <- sum((Y_hat-mean(Y))^2)
ssr
ssto <- sse + ssr
ssto
```

ANOVA Table

Source of Variation    SS          df         MS
-------------------   ---------   ------    ---------------    
Regression            SSR=5298     1         MSR=SSR/1=5298
Error                 SSE=146     n-2=14     MSE=SSE/(n-2)=10.43
Total                 SSTO=5444   n-1=15 

## (b)

$H_0:\beta_1=0$ (there is no association between X and Y)

$H_a:\beta_1\neq0$ (there is linear association between X and Y)

$F^{\ast}=MSR/MSE$

Decision rule: 

If $F^{\ast}\le{F(1-\alpha;1,n-2)}$, conclude $H_0$

If $F^{\ast}>{F(1-\alpha;1,n-2)}$, conclude $H_a$

```{r}
msr <- ssr
mse <- sse/(n-2)
ftest <- msr/mse
ftest
```

$F^{\ast}=506.5>F(1-0.01;1,14)=8.862$

We conclude $H_a$, there is a linear association.

## (c)
```{r}
plot(X, Y-Y_hat, ylim=c(-25, 25))
plot(X, Y_hat-mean(Y), ylim=c(-25, 25))
```

SSR is the larger component of SSTO
It means $R^2$ is close to 1

## (d)
```{r}
r_sq <- ssr/ssto
r_sq
correlation <- sqrt(r_sq)
correlation
```

$R^2=0.9731$

$r=0.9865$

# 2.41

No. X here is fixed, not a random variable. 
Spearman Rank Correlation Coefficient (RHO) is used for two random variables Y1 and Y2 
whose joint distribution differ considerably from bivariate normal distribution.
So it's not useful here.

# 3.14

## (a)

j is the index for different level of X, i is the index for different replicas at the same level of X

$H_0:Y_{ij}=\beta_0+\beta_1X_j+\epsilon_{ij}$ (linear model is a good fit for the data)

$H_a:Y_{ij}=\mu_j+\epsilon_{ij}$ (linear model is not a good fit, more complex model is needed)

Decision rule: 

If $F^{\ast}\le{F(1-\alpha;c-2,n-c)}$, conclude $H_0$

If $F^{\ast}>{F(1-\alpha;c-2,n-c)}$, conclude $H_a$

```{r}
Y1 <- c(199, 205, 196, 200)
Y2 <- c(218, 220, 215, 223)
Y3 <- c(237, 234, 235, 230)
Y4 <- c(250, 248, 253, 246)
sspe <- sum((Y1-mean(Y1))^2)+sum((Y2-mean(Y2))^2)+sum((Y3-mean(Y3))^2)+sum((Y4-mean(Y4))^2)
c <- 4
ftest_fit <- ((sse-sspe)/(c-2))/(sspe/(n-c))
ftest_fit
```

$F^{\ast}=0.8237<F(0.01; 2, 12)=6.927$

We conclude $H_0$, linear model is a good fit.

## (b)
Equal number of replicas is advantageous, because it makes each level of X equally important.
But if we know the relation is linear, assigning more samples at extreme points will increase the power of the test.

## (c)
No, lack of fit test doesn't indicate what regression function is appropriate. 
One apporach is using a more complex model, such as quadratic regression function or logarithmic regression. We can plot the residuals to see which function is best.

# 6.9
## (a)

```{r}
library(ggplot2)
retails <- read.table("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/5th/KutnerData/Chapter%20%206%20Data%20Sets/CH06PR09.txt", col.names=c("Y", "X1", "X2", "X3"))

X1 <- retails$X1
X2 <- retails$X2
X3 <- retails$X3

Y <- retails$Y

boxplot(X1, main="X1")
boxplot(X2, main="X2")
hist(X1)
hist(X2)
hist(Y)
```


We can see from the boxplots that there are outliers in X1 and X2.

From the histgraph, we can see there is a gap (4600-4800) in Y.

## (b)
```{r}
plot(X1, type="o")
plot(X2, type="o")
plot(X3, type="p")
```

X1 decreases from week 0-20, then increases from week 20-40, and reaches highest from week 40-50.

For X2, there seem to be cycles every 5-6 weeks, and a general trend of increase throughout the year.

## (c)
```{r}
pairs(retails[, 2:4])
cor(retails[, 2:4])
```

From the scatter plot matrix and correlation matrix, we can see there isn't much correlation between predictor variables.

# 6.10
## (a)

```{r}
fit <- lm(Y ~ X1 + X2 + X3)
summary(fit)
b0 <- fit$coefficients[1]
b0
b1 <- fit$coefficients[2]
b1
b2 <- fit$coefficients[3]
b2
b3 <- fit$coefficients[4]
b3
```

$Y_{hat}=4149.89+0.000787X_1-13.166X_2+623.55X_3$

b1 is the change of E{Y} with a unit increase in $X_1$ when $X_2$ and $X_3$ are held constant

b2 is the change of E{Y} with a unit increase in $X_2$ when $X_1$ and $X_3$ are held constant

b3 is the change of E{Y} with a unit increase in $X_3$ when $X_1$ and $X_2$ are held constant

## (b)
```{r}
resid <- resid(fit)
boxplot(resid)
```

From the boxplot we can see that the residuals are not skewed, and there are no outliers.

## (c)
```{r}
Y_hat <- b0+b1*X1+b2*X2+b3*X3
plot(Y_hat, resid)
plot(X1, resid)
plot(X2, resid)
plot(X3, resid)
plot(X1*X2, resid)
qqnorm(resid)
qqline(resid)
```

It looks like there's no systematic error, residuals have 0 mean. residual variance is larger when X1 is small, but there are more samples with small X1, that could explain the variance. From quantile-quantile plot, it looks like residuals are close to normal distribution.


## (d)
```{r}
plot(resid, type="o")
```

The residuals seem to be independent, there isn't any indication that the error terms are correlated.

# 6.11
## (a)
$H_0:\beta_1=\beta_2=\beta_3=0$ 

$H_a:$ at least one of $\beta_1,\beta_2,\beta_3$ is not zero

$F^{\ast}=MSR/MSE$

Decision rule: 

If $F^{\ast}\le{F(1-\alpha;p-1,n-p)}$, conclude $H_0$

If $F^{\ast}>{F(1-\alpha;p-1,n-p)}$, conclude $H_a$

```{r}
sse <- sum((Y-Y_hat)^2)
ssr <- sum((Y_hat-mean(Y))^2)
n <- length(Y)
p <- 4
msr <- ssr/(p-1)
mse <- sse/(n-p)
ftest <- msr/mse
ftest
```
$F^{\ast}=35.34>F(0.05; 3, 48)=2.8$

We conclude $H_a$, at least one of $\beta_1,\beta_2,\beta_3$ is not zero.

```{r}
pvalue <- pf(ftest, 3, 48, lower.tail=FALSE)
pvalue
```

## (c)
```{r}
r_sq <- ssr/(ssr+sse)
r_sq
```

$R^2$ here measures the proportionate reduction of total variation in Y associated with the use of the set of X variables X1, X2, X3

# 6.14
## (a)
```{r}
b <- c(b0, b1, b2, b3)
Xh <- matrix(c(1, 282000, 7.10, 0), nrow=1, ncol=4)
Yh <- sum(Xh*b)
bvar <- vcov(fit)
Yh_var <- Xh %*% bvar %*% t(Xh)
ttest <- qt(0.975, df=48)
predmean_var <- mse/3 + Yh_var
predmean_sd <- sqrt(predmean_var)
Yh + ttest*predmean_sd
Yh - ttest*predmean_sd
```

95 percent prediction interval for E{Yh} is [4106, 4451]

## (b)

95 percent prediction interval for the total 3 shipments is 3*[4106, 4451]= [12318, 13353]

# 7.13
```{r}
ssto <- sse+ssr
fit_y_x1 <- lm(Y ~ X1)
anova(fit_y_x1)
fit_y_x2 <- lm(Y ~ X2)
anova(fit_y_x2)
fit_x1_x2 <- lm(X1 ~ X2)
anova(fit_x1_x2)
fit_y_x1x2 <- lm(Y ~ X1 + X2)
anova(fit_y_x1x2)
fit_y_x1x3 <- lm(Y ~ X1 + X3)
anova(fit_y_x1x3)
fit_all <- lm(Y ~ X1 + X2 + X3)
anova(fit_all)
```

$R_{Y1}^2=\frac{SSR(X_1)}{SSTO(Y)}=\frac{136366}{3162136}=0.043$

$R_{Y2}^2=\frac{SSR(X_2)}{SSTO(Y)}=\frac{11395}{3162136}=0.0036$

$R_{12}^2=\frac{SSR(X_2)}{SSTO(X_1)}=\frac{1.1231e09}{1.1231e09+1.5470e11}=0.0072$

$R_{Y1|2}^2=\frac{SSR(X_1|X_2)}{SSE(X_2)}=\frac{SSE(X_2)-SSE(X_1,X_2)}{SSE(X_2)}=\frac{3150741-3020044}{3150741}=0.04148$

$R_{Y2|1}^2=\frac{SSR(X_2|X_1)}{SSE(X_1)}=\frac{SSE(X_1)-SSE(X_1,X_2)}{SSE(X_1)}=\frac{3025770-3020044}{3025770}=0.00189$

$R_{Y2|13}^2=\frac{SSR(X_2|X_1,X_3)}{SSE(X_1,X_3)}=\frac{SSE(X_1,X_3)-SSE(X_1,X_2,X_3)}{SSE(X_1,X_3)}=\frac{992204-985530}{992204}=0.00673$

$R^2=\frac{SSR(X_1,X_2,X_3)}{SSTO}=\frac{SSTO-SSE(X_1,X_2,X_3)}{SSTO}=\frac{2176606}{3162136}=0.6883$

# 7.17
## (a)
```{r}
cor_trans <- function(X){
  n <- length(X)
  sd_X <- sqrt(sum((X-mean(X))^2)/(n-1))
  X_star <- (X-mean(X))/sd_X/sqrt(n-1)
  return(X_star)
}

Y_star <- cor_trans(Y)
X1_star <- cor_trans(X1)
X2_star <- cor_trans(X2)
X3_star <- cor_trans(X3)

fit_star <- lm(Y_star ~ X1_star + X2_star + X3_star)
summary(fit_star)
```

$Y^*=0.1747X_1^*-0.04639X_2^*+0.8079X_3^*$

## (b)
```{r}
fit_x2_x1 <- lm(X2 ~ X1)
anova(fit_x2_x1)
fit_x1_x3 <- lm(X1 ~ X3)
anova(fit_x1_x3)
fit_x2_x3 <- lm(X2 ~ X3)
anova(fit_x2_x3)
```

$R_{12}^2=R_{21}^2=\frac{0.283}{0.283+38.977}=0.0072$

$R_{13}^2=R_{31}^2=\frac{3.2482e08}{3.2482e08+1.5549e11}=0.0021$

$R_{23}^2=R_{32}^2=\frac{0.505}{0.505+38.755}=0.0129$

Yes, it is meaningful to consider the standardized regressioin coefficient to reflect the effect of one predictor variable when the others are held constant. Since we can see from the coefficients of determination between all pairs of predictor variables, that there's no strong correlation between any pair of predictor variables.

## (c)
```{r}
para <- function(Y, X, para_star){
  n <- length(X)
  sd_X <- sqrt(sum((X-mean(X))^2)/(n-1))
  sd_Y <- sqrt(sum((Y-mean(Y))^2)/(n-1))
  org_para <- para_star*sd_Y/sd_X
  return(org_para)
}
para(Y, X1, 0.1747)
para(Y, X2, -0.04639)
para(Y, X3, 0.8079)
```

When transforming the estimated standardized regression coefficients back to the original model, the result is the same as obtained from 6.10(a)

# 7.25
## (a)
```{r}
fit_y_x1 <- lm(Y ~ X1)
summary(fit_y_x1)
```

$Y=4080+0.000935X_1$

## (b)
The estimated regression coefficient for X1 is 0.000935, which is only slightly larger than 0.000787 (obtained from 6.10(a)). It means adding X2 and X3 in the model doesn't affect the coefficient of X1 much.

## (c)
$SSR(X_1)=136366$

$SSR(X_1|X_2)=SSE(X_2)-SSE(X_1,X_2)=3150741-3020044=130697$

The difference between $SSR(X_1)$ and $SSR(X_1|X_2)$ is very small

## (d)

From the correlation matrix, we have $r_{12}=0.0849$, which means X1 and X2 are not correlated, it matches with the result of part (b) and part (c)
