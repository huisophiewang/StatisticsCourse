---
title: "hw8"
author: Hui Sophie Wang
output: html_document
---
# 14.20
## (a)
```{r}
data <- read.table("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/5th/KutnerData/Chapter%2014%20Data%20Sets/CH14PR14.txt", col.names=c("Y", "X1", "X2", "X3"))
Y <- data$Y
X1 <- data$X1
X2 <- data$X2
X3 <- data$X3

fit <- glm(Y~X1+X2+X3, family=binomial)
summary(fit)
```


For Bonferroni Confidence Interval $B=Z^{1-\alpha/2g}$, $\alpha=0.1$, $g=2$, so $B=Z^{0.975}=1.96$

$s^2\{b_1\}=0.0304$

CI of $\beta_1$ is $0.073\pm1.96*0.0304=[0.013, 0.133]$

CI of $exp(30\beta_1)$ is $[1.48, 54]$

$s^2\{b_2\}=0.0335$

CI of $\beta_2$ is $-0.099\pm1.96*0.0335=[-0.165, -0.0333]$

CI of $exp(25\beta_2)$ is $[0.016, 0.435]$

Interpretation: With family confidence coefficient 0.9, we conclude that age odds ratio for males whose ages differ by 30 years falls between [1.48, 54], and health awareness index odds ratio for males whose health awareness index differ by 25 falls between [0.016, 0.435]

## (b)
Hypothesis:

$H_0:\beta_3=0$ 

$H_a:\beta_3\neq{0}$ 

Decision rule:

If $|z^*|<z^{0.975}$, conclude $H_0$, else, conclude $H_a$

$z^*=b_3/s^2\{b_3\}=0.4334/0.5218=0.83$

$z^{0.975}=1.96$, ince $z^*<z^{0.975}$, we conclude $H_0$

p-value is 0.406

## (c)
Hypothesis:

$H_0:\beta_3=0$ (Reduced Model)

$H_a:\beta_3\neq{0}$ (Full Model)

Decision rule:

If $G^2<\chi^2(0.95, 1)$, conclude $H_0$, else, conclude $H_a$


```{r}
pi_prime <- cbind(1, X1, X2, X3) %*% coef(fit)
log_lf <- sum(Y*pi_prime) - sum(log(1+exp(pi_prime)))

fit_r <- glm(Y~X1+X2, family=binomial) 
pi_prime_r <- cbind(1, X1, X2) %*% coef(fit_r)
log_lr <- sum(Y*pi_prime_r) - sum(log(1+exp(pi_prime_r)))

(-2)*(log_lr-log_lf)

```

$G^2=0.7$

$\chi^2(0.95, 1)=3.841$, since $G^2<\chi^2(0.95, 1)$, we conclude $H_0$

p-value is 0.403

The p-value obtained from likelihood ratio test and Wald test are very close. And they lead to the same conclusion.

## (d)

$X\beta=\beta_0+X_1\beta_1+X_2\beta_2+X_1^2\beta_3+X_2^2\beta_4+X_1X_2\beta_5$

Hypothesis:

$H_0:\beta_3=\beta_4=\beta_5=0$ (Reduced Model)

$H_a$ :not all the $\beta$ in $H_0$ is zero (Full Model)

Decision rule:

If $G^2<\chi^2(0.95, 3)$, conclude $H_0$, else, conclude $H_a$

```{r}
X11 <- X1^2
X22 <- X2^2
X12 <- X1*X2
X21 <- X2*X1
fit2 <- glm(Y~X1+X2+X11+X22+X12, family=binomial)
pi_prime2 <- cbind(1, X1, X2, X11, X22, X12) %*% coef(fit2)
log_l2 <- sum(Y*pi_prime2) - sum(log(1+exp(pi_prime2)))

(-2)*(log_lr-log_l2)
```

$G^2=1.534$

$\chi^2(0.95, 1)=7.815$, since $G^2<\chi^2(0.95, 3)$, we conclude $H_0$, second order terms should not be added.

p-value is 0.674

# 14.22
## (a)
$X\beta=\beta_0+X_1\beta_1+X_2\beta_2+X_3\beta_3+X_1^2\beta_4+X_2^2\beta_5+X_1X_2\beta_6$
```{r}
fit_x1 <- glm(Y~X1, family=binomial)
summary(fit_x1)
fit_x2 <- glm(Y~X2, family=binomial)
summary(fit_x2)
fit_x3 <- glm(Y~X3, family=binomial)
summary(fit_x3)
fit_x11 <- glm(Y~X11, family=binomial)
summary(fit_x11)
fit_x22 <- glm(Y~X22, family=binomial)
summary(fit_x22)
fit_x12 <- glm(Y~X12, family=binomial)
summary(fit_x12)
```

Step 1: X11 is added

```{r}
fit_x11x1 <- glm(Y~X11+X1, family=binomial)
summary(fit_x11x1)
fit_x11x2 <- glm(Y~X11+X2, family=binomial)
summary(fit_x11x2)
fit_x11x3 <- glm(Y~X11+X3, family=binomial)
summary(fit_x11x3)
fit_x11x22 <- glm(Y~X11+X22, family=binomial)
summary(fit_x11x22)
fit_x11x12 <- glm(Y~X11+X12, family=binomial)
summary(fit_x11x12)
```

Step 2: X2 is added

```{r}
fit_x11x2x1 <- glm(Y~X11+X2+X1, family=binomial)
summary(fit_x11x2x1)
fit_x11x2x3 <- glm(Y~X11+X2+X3, family=binomial)
summary(fit_x11x2x3)
fit_x11x2x22 <- glm(Y~X11+X2+X22, family=binomial)
summary(fit_x11x2x22)
fit_x11x2x12 <- glm(Y~X11+X2+X12, family=binomial)
summary(fit_x11x2x12)
```
Step 3: no predictor is added, stop

Final model contains X2, X11

## (b)
```{r}
fit_all <- glm(Y~X1+X2+X3+X11+X22+X12, family=binomial)
summary(fit_all)
```
Step 1: X1 is removed

```{r}
fit_no_x1 <- glm(Y~X2+X3+X11+X22+X12, family=binomial)
summary(fit_no_x1)
```
Step 2: X11 is removed

```{r}
fit_no_x1x11 <- glm(Y~X2+X3+X22+X12, family=binomial)
summary(fit_no_x1x11)
```
Step 3: X3 is removed

```{r}
fit_no_x1x11x3 <- glm(Y~X2+X22+X12, family=binomial)
summary(fit_no_x1x11x3)
```
Step 4: X22 is removed

```{r}
fit_no_x1x11x3x22 <- glm(Y~X2+X12, family=binomial)
summary(fit_no_x1x11x3x22)
```
Step 5: no predictor is removed, stop

Final model contains X2 and X12.

The result is not the same as (a)
## (c)
## (d)

# 14.28
## (a)
```{r}
fit_x1x2 <- glm(Y~X1+X2, family=binomial)
summary(fit_x1x2)
pi_prime <- cbind(1, X1, X2) %*% coef(fit_x1x2)

res <- cbind(Y, pi_prime)
res <- res[order(res[,2], res[,1]),]

for(j in c(1:7)){
  start <- (j-1)*20+1
  end <- start + 19
  m <- (res[start,2]+res[end,2])/2
  print(m)
  p <- 1.0/(1+exp(-m))
  print(p)
  count <- sum(res[start:end,1])
  print(count)
}
m8 <- (res[141,2]+res[159,2])/2
m8
1.0/(1+exp(-m8))
sum(res[141:159,1])
```
group j   $n_j$    $\hat{\pi_j}$  $E_{j1}$  $O_{j1}$  $E_{j0}$  $O_{j0}$  
--------  ------  --------------  --------  --------  --------  --------
1          20      0.008           0.16      0         19.84      20
2          20      0.026           0.52      1         19.48      19
3          20      0.050           1         0         19         20
4          20      0.079           1.58      2         18.42      18
5          20      0.127           2.54      2         17.46      18
6          20      0.180           3.6       7         16.4       13
7          20      0.25            5         2         15         18
8          19      0.682           13.6      10        6.4        9

```{r}
m <- c(-4.82, -3.63, -2.96, -2.46, -1.93, -1.52, -1.09, 0.765)
p <- c(0.008, 0.026, 0.05, 0.079, 0.127, 0.18, 0.25, 0.682)
plot(m, p, xlab="midpoints", ylab="estimated p")
```

## (b)
Hypothesis:

$H_0:E[Y]=1/(1+exp(-X\beta))$

$H_a:E[Y]\neq{1/(1+exp(-X\beta))}$

Decision rule:

If $\chi^2<\chi^2(0.95, 8-3)$, conclude $H_0$, else, conclude $H_a$

$\chi^2=10.26$

$\chi^2(0.95, 8-3)=11.07$, Since $\chi^2<\chi^2(0.95, 5)$, we conclude $H_0$

p-value is 0.068

## (c)
```{r}
#plot( residuals(fit_x1x2, type="deviance") ~ predict(fit_x1x2, type="response"), xlab=expression(hat(pi)), ylab="Deviance Residual")

pi <- 1.0/(1+exp(-pi_prime))
dev <- (-2)*(Y*log(pi)+(1-Y)*log(1-pi))
sign <- ifelse(Y-pi>0, 1, -1)
dev <- sign*sqrt(dev)
plot(pi, dev)
```


# 14.36
## (a)
```{r}
fit_all <- glm(Y~X1+X2+X3, family=binomial)
Xh <- cbind(1, 65, 50, 0)
pi_prime <- sum(coef(fit_all)*Xh)
sd <- sqrt(Xh %*% vcov(fit_all) %*% t(Xh))
```

For Bonferroni Confidence Interval $B=Z^{1-\alpha/2g}$, $\alpha=0.1$, $g=1$, so $B=Z^{0.95}=1.645$

L = -1.395-1.645*0.402 = -2.056

U = -1.395+1.645*0.402 = -0.735

$L^*$=1/(1+exp(-L)) = 0.113

$U^*$=1/(1+exp(-U)) = 0.324

## (b)
```{r}
fit_all <- glm(Y~X1+X2+X3, family=binomial)
pi_prime <- cbind(1, X1, X2, X3) %*% coef(fit_all)
pi <- 1.0/(1+exp(-pi_prime))
Y_hat <- ifelse(pi>0.05, 1, 0)
sum(as.numeric(Y==0 & Y_hat==0))
sum(as.numeric(Y==0 & Y_hat==1))
sum(as.numeric(Y==1 & Y_hat==0))
sum(as.numeric(Y==1 & Y_hat==1))
```
cutoff=0.05

Truth   $\hat{Y}=0$    $\hat{Y}=1$    total
-----  -------------  -------------  -------
Y=0       50           85             135
Y=1       1            23             24
total     51           108            159

$total_error_rate$=(1+85)/159=0.54
$Pr(\hat{Y}=0|Y=1)$=1/24=0.042
$Pr(\hat{Y}=1|Y=0)$=85/135=0.63

```{r}
Y_hat <- ifelse(pi>0.1, 1, 0)
sum(as.numeric(Y==0 & Y_hat==0))
sum(as.numeric(Y==0 & Y_hat==1))
sum(as.numeric(Y==1 & Y_hat==0))
sum(as.numeric(Y==1 & Y_hat==1))
```
cutoff=0.1

Truth   $\hat{Y}=0$    $\hat{Y}=1$    total
-----  -------------  -------------  -------
Y=0       80           55             135
Y=1       3            21             24
total     83           76             159

$total_error_rate$=(3+55)/159=0.365
$Pr(\hat{Y}=0|Y=1)$=3/24=0.125
$Pr(\hat{Y}=1|Y=0)$=55/135=0.407

```{r}
Y_hat <- ifelse(pi>0.15, 1, 0)
sum(as.numeric(Y==0 & Y_hat==0))
sum(as.numeric(Y==0 & Y_hat==1))
sum(as.numeric(Y==1 & Y_hat==0))
sum(as.numeric(Y==1 & Y_hat==1))
```
cutoff=0.15

Truth   $\hat{Y}=0$    $\hat{Y}=1$    total
-----  -------------  -------------  -------
Y=0       97           38             135
Y=1       4            20             24
total     101          58             159

$total_error_rate$=(4+38)/159=0.264
$Pr(\hat{Y}=0|Y=1)$=4/24=0.167
$Pr(\hat{Y}=1|Y=0)$=38/135=0.281

```{r}
Y_hat <- ifelse(pi>0.4, 1, 0)
sum(as.numeric(Y==0 & Y_hat==0))
sum(as.numeric(Y==0 & Y_hat==1))
sum(as.numeric(Y==1 & Y_hat==0))
sum(as.numeric(Y==1 & Y_hat==1))
```
cutoff=0.2

Truth   $\hat{Y}=0$    $\hat{Y}=1$    total
-----  -------------  -------------  -------
Y=0       113          22             135
Y=1       10           14             24
total     123          36             159

$total_error_rate$=(10+22)/159=0.201
$Pr(\hat{Y}=0|Y=1)$=10/24=0.417
$Pr(\hat{Y}=1|Y=0)$=22/135=0.163

## (c)
cutoff=0.2 minimize the total error rate

```{r}
library(ROCR)
res <- predict(fit_all, type="response")
pred <- prediction(res, labels=Y)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=F)
unlist(attributes(performance(pred, "auc"))$y.values)
```

Area under ROC curve is 0.822, which is pretty good, we conclude the model has high predictive power.

## (d)
Should test on a validation data set to verify.

# 11.28
## (a)

```{r}
X <- c(35, 35, 40, 40, 45, 45, 50, 50, 55, 55, 60, 60)
Y <- c(22, 20, 28, 31, 37, 38, 41, 39, 34, 37, 27, 30)

XX <- X^2
fit <- lm(Y~X+XX)
summary(fit)

plot(X, Y)
#lines(X, predict(fit), col="blue")
curve(-182.6+8.983*x-0.09107*x^2, 35, 60, col = "blue", add = TRUE)

```

The quadratic regression function appear to be a good fit.

## (b)
```{r}
b1 <- fit$coefficients[2]
b11 <- fit$coefficients[3]

X_max <- (-0.5)*b1/b11
X_max

Y_max <- sum(c(1, X_max, X_max^2)*coef(fit))
Y_max
```

$\hat{X}_{max}=49.32$

$E[Y]$ given $\hat{X}_{max}$ is 38.94

## (c)
```{r}
Y_hat <- predict(fit)
e <- Y-predict(fit)

n <- 1000
maxs <- vector(mode='numeric',length=n)
for(i in 1:n){
  Y_new <- Y_hat + sample(e, replace = TRUE)
  fit_new <- lm(Y_new~X+XX)
  b1_new <- fit_new$coefficients[2]
  b11_new <- fit_new$coefficients[3]
  maxs[i] <- (-0.5)*b1_new/b11_new
}
```

## (d)
```{r}
hist(maxs)
```

The bootstrap sampling distribution of $\hat{X}_{max}^*$ appear to approximate normal distribution

## (e)
```{r}
xmax.05 <- quantile(maxs, 0.05)
xmax.95 <- quantile(maxs, 0.95)
xmax_var <- var(maxs)
d1 <- X_max - xmax.05
d2 <- xmax.95 - X_max
L <- X_max - d2
U <- X_max + d1
```

90 percent bootstrap confidence interval for Xmax using reflection method is [48.814, 49.820]


